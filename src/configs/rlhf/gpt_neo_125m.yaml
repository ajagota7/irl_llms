# Configuration for GPT-Neo-125M RLHF

# Model settings
model_name: "EleutherAI/gpt-neo-125M"
hub_model_id: "gpt-neo-125m-detox"

# Training parameters
num_train_epochs: 100
batch_size: 128
mini_batch_size: 32
forward_batch_size: 8
gradient_accumulation_steps: 8
learning_rate: 1e-5
lr_scheduler_type: "cosine"
warmup_ratio: 0.1

# PPO specific settings
ppo_epochs: 4
init_kl_coef: 0.2
target: 6
horizon: 10000
cliprange: 0.2
cliprange_value: 0.2
vf_coef: 0.1
gamma: 1.0
lam: 0.95
adap_kl_ctrl: true
use_score_norm: true
ratio_threshold: 10.0

# Dataset and reward settings
query_dataset: "allenai/real-toxicity-prompts"
reward_model: "facebook/roberta-hate-speech-dynabench-r4-target"
input_min_text_length: 15
input_max_text_length: 20
output_min_length: 15
output_max_length: 20

# Toxicity threshold for filtering prompts (higher means more toxic)
toxicity_threshold: 0.3

# Generation parameters
do_sample: true
top_k: 0.0
top_p: 1.0