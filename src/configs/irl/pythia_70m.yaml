# Configuration for Pythia-70M IRL

# Mode to run
mode: "train"  # "generate_dataset", "train", or "all"

# Dataset settings
dataset:
  original_model_name: "EleutherAI/pythia-70M"
  detoxified_model_name: "ajagota71/pythia-70M-detox-epoch-100"
  original_dataset_path: null
  detoxified_dataset_path: null
  # custom_dataset_name: "my_custom_dataset"  # Uncomment to use custom name instead of default

model:
  reward_model_base: "EleutherAI/pythia-70M"
  use_half_precision: false
  num_unfrozen_layers: 1

training:
  # IRL method options:
  # - "max_margin": Symmetric margin loss (default)
  # - "asymmetric_margin": Asymmetric margin with different penalties for each direction
  # - "confidence_margin": Dynamic margin that increases with prediction confidence
  # - "max_entropy": Maximum entropy IRL
  irl_method: "max_margin"
  
  # Parameters for max_margin (symmetric)
  margin: 0.1
  
  # Parameters for asymmetric_margin
  # positive_penalty: 1.0  # penalty when detoxified > original (good case)
  # negative_penalty: 2.0  # penalty when original > detoxified (bad case)
  
  # Parameters for confidence_margin
  # base_margin: 0.1       # base margin value
  # confidence_factor: 0.5 # factor to scale the dynamic margin
  
  # Parameters for max_entropy
  # temperature: 0.1
  
  learning_rate: 1e-5
  epochs: 20
  batch_size: 8
  eval_interval: 5

output:
  repo_name_prefix: "irl-pythia-70m"
  save_checkpoints: true
  checkpoint_interval: 5  # Save checkpoints at epoch 0 and every 5 epochs
  push_checkpoints_to_hub: true  # Push checkpoints to hub